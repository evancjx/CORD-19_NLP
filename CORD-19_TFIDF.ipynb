{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "haKIC2dMLprZ"
   },
   "source": [
    "### Initalize Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zC1xsv8uLpra"
   },
   "source": [
    "If you're running this script on Google Colab<br>\n",
    "Mount your Google drive: \n",
    "1. Click on the folder icon on the left\n",
    "2. Click Mount Drive\n",
    "3. The root directory would be /content/\n",
    "```\n",
    "# your Google Drive folder would be at:\n",
    "/content/drive/My Drive/\n",
    "```\n",
    "\n",
    "Change working directory:<br>\n",
    "1. Run this command:\n",
    "```\n",
    "%cd /content/drive/My Drive/<your folder>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1591053891001,
     "user": {
      "displayName": "Evan Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhLwuv1HrLfJCuFO3Jv9_dkRwGwd90aHRAVsWIh7Q=s64",
      "userId": "01810893482679501710"
     },
     "user_tz": -480
    },
    "id": "yizVYgqQS-nh",
    "outputId": "f1a7313b-bf80-4419-8381-8718a984dc11"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/My Drive/Data Science/Covid-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download optional (required) files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download nltk stopwords to use Stopwords\n",
    "```\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "Download nltk wordnet to use WordNetLemmatizer:\n",
    "```\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "```\n",
    "Download nltk punkt to use Punkt Sentence Tokenizer\n",
    "```\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all\n",
    "Each paper are in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "folder = 'raw_data/comm_use_subset'\n",
    "filenames = os.listdir(f'./{folder}')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from src.covid_19_tp import authors_name, body_text, format_bib\n",
    "data = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            'paper_id': file['paper_id'],\n",
    "            'title': file['metadata']['title'],\n",
    "            'authors': authors_name(file['metadata']['authors'], affiliation=True),\n",
    "\n",
    "            'abstract': body_text(file['abstract']),\n",
    "            'text': body_text(file['body_text']),\n",
    "\n",
    "            'bibliography': format_bib(file['bib_entries'])\n",
    "        }\n",
    "        for file in [\n",
    "            json.load(open(f'{folder}/{filename}'))\n",
    "            for filename in filenames\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NLP functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_preprocessing import spacy_NLP, nltk_NLP\n",
    "spacy_tokenizer = spacy_NLP('en_core_web_sm').tokenize_API()\n",
    "nlp_tokenizer = nltk_NLP().tokenize_API()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nlp_custom_tokenizer = nltk_NLP(stemming=PorterStemmer, lemmatisation=WordNetLemmatizer).custom_API()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Corpus from dataset\n",
    "Save the corpus as pickle file to save time in the future; Load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_preprocessing import STOP_WORDS, text_preprocess\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "corpus = [\n",
    "    text_preprocess(text, tokenizer=spacy_tokenizer, stopwords=STOP_WORDS)\n",
    "    for text in tqdm(list(data['title'] + ' ' + data['abstract'] + ' ' + data['text']))\n",
    "]\n",
    "\n",
    "filename = '_'.join(folder.split('/'))\n",
    "folder = 'processed_data'\n",
    "\n",
    "from os.path import isdir\n",
    "from os import mkdir\n",
    "if not isdir(f'./{folder}'): mkdir(f'./{folder}') # Create folder if it does not exist\n",
    "\n",
    "import pickle\n",
    "with open(f'{folder}/{filename}_corpus.pkl', 'wb') as output:\n",
    "    pickle.dump(corpus, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Corpus from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "folder = 'processed_data'\n",
    "filename = 'raw_data_comm_use_subset_corpus'\n",
    "with open(f'./{folder}/{filename}.pkl', 'rb') as f:\n",
    "    corpus = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from src.tf_idf import corpus_tf_idf\n",
    "\n",
    "corpus_doc_tf_idf, term_doc_freq = corpus_tf_idf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    corpus_doc_tf_idf: list of td-idf scores (terms: score) of each documents\n",
    "    score:\n",
    "        Low = frequent terms\n",
    "        High = rare terms\n",
    "'''\n",
    "corpus_doc_tf_idf[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    term_doc_freq: a dict (key: value pairs) of a term and it's count of occurrence in different documents\n",
    "'''\n",
    "term_doc_freq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
