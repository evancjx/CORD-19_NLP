{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initalize Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running this script on Google Colab<br>\n",
    "Mount your Google drive: \n",
    "1. Click on the folder icon on the left\n",
    "2. Click Mount Drive\n",
    "3. The root directory would be /content/\n",
    "```\n",
    "# your Google Drive folder would be at:\n",
    "/content/drive/My Drive/\n",
    "```\n",
    "\n",
    "Change working directory:<br>\n",
    "1. Run this command:\n",
    "```\n",
    "%cd /content/drive/My Drive/<your folder>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1590985315634,
     "user": {
      "displayName": "Evan Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhLwuv1HrLfJCuFO3Jv9_dkRwGwd90aHRAVsWIh7Q=s64",
      "userId": "01810893482679501710"
     },
     "user_tz": -480
    },
    "id": "zJMtNVCDEgHT",
    "outputId": "50b72a57-268c-45e4-e820-4e22a81a5417"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/My Drive/Data Science/Covid-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download optional (required) files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download nltk stopwords to use Stopwords\n",
    "```\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "Download nltk wordnet to use WordNetLemmatizer:\n",
    "```\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "```\n",
    "Download nltk punkt to use Punkt Sentence Tokenizer\n",
    "```\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NLP text preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWSm9D-YRRi0"
   },
   "outputs": [],
   "source": [
    "from src.text_preprocessing import nltk_NLP, spacy_NLP, STOP_WORDS, text_preprocess\n",
    "spacy_tokenizer = spacy_NLP('en_core_web_sm').tokenize_API()\n",
    "nlp_tokenizer = nltk_NLP().tokenize_API()\n",
    "\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# nlp_custom_tokenizer = nltk_NLP(stemming=PorterStemmer, lemmatisation=WordNetLemmatizer).custom_API()\n",
    "\n",
    "text_prep = lambda text: text_preprocess(text, tokenizer=spacy_tokenizer, stopwords=STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and pre-process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 222305,
     "status": "error",
     "timestamp": 1590988231264,
     "user": {
      "displayName": "Evan Chang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhLwuv1HrLfJCuFO3Jv9_dkRwGwd90aHRAVsWIh7Q=s64",
      "userId": "01810893482679501710"
     },
     "user_tz": -480
    },
    "id": "leOO69R-Gp1u",
    "outputId": "a3f735b3-64ec-4e71-a383-c9f88530633a"
   },
   "outputs": [],
   "source": [
    "from src.text_preprocessing import STOP_WORDS, text_preprocess\n",
    "\n",
    "from os import walk as dir_list\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "folder = 'raw_data'\n",
    "\n",
    "data_title_abstract = [\n",
    "    {\n",
    "        'paper_id':file['paper_id'], \n",
    "        'title':file['metadata']['title'], \n",
    "        'abstract': text_prep(\n",
    "            ''.join([row['text']+'\\n' for row in file['abstract']])\n",
    "        )\n",
    "    }\n",
    "    for subdir, dirs, files in dir_list(f'./{folder}')\n",
    "    for file in tqdm(\n",
    "        [\n",
    "            json.load(open(f'{subdir}/{file}'))\n",
    "            for file in tqdm(files, desc=f'Loading all files in {subdir}')\n",
    "        ], desc=f'Reading individual files in {subdir}'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_title_abstract = pd.DataFrame(data_title_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load Data to and from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "folder = 'processed_data'\n",
    "filename = 'data_entire_title_abstract'\n",
    "with open(f'./{folder}/{filename}.pkl', 'wb') as output:\n",
    "    pickle.dump(data_title_abstract, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "folder = 'processed_data'\n",
    "filename = 'data_entire_title_abstract'\n",
    "with open(f'./{folder}/{filename}.pkl', 'rb') as f:\n",
    "    data_title_abstract = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(data_title_abstract.loc[:, 'abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct TF-IDF using skLearn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tf_idf import sklearn_TFIDF\n",
    "\n",
    "sk_tfidf = sklearn_TFIDF()\n",
    "sk_tfidf.tfidf_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column (keywords)\n",
    "data_title_abstract = data_title_abstract.reindex(columns=list(data_title_abstract.columns)+['keywords'])\n",
    "data_title_abstract['keywords'] = data_title_abstract['abstract'].apply(\n",
    "    lambda text: sk_tfidf.get_text_keywords(text)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new DataFrame as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "folder = 'processed_data'\n",
    "filename = 'data_entire_title_abstract_keywords'\n",
    "with open(f'./{folder}/{filename}.pkl', 'wb') as output:\n",
    "    pickle.dump(data_title_abstract, output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOOQ7dyp6X640+a4Um1FYtX",
   "mount_file_id": "1dn-zqrePSdsReM8Rq-_FE2iyKF4UeZIT",
   "name": "Text Analysis on CORD-19.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
